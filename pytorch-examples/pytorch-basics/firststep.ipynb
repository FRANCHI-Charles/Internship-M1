{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.9713e-35,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00, -1.8962e-20,  4.0769e-41]]])\n",
      "tensor([[[0.8022, 0.0264, 0.3213],\n",
      "         [0.2271, 0.8301, 0.1147]]], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "idecide = True\n",
    "\n",
    "print(t.empty(1,2,3))\n",
    "print(t.rand(1,2,3, dtype = t.float16 if idecide else t.double))\n",
    "t.zeros(2,2)\n",
    "t.ones(2,2)\n",
    "\n",
    "x = t.tensor([[1,2,3],[4,5,6]])\n",
    "x.size() #np.shape\n",
    "y = x.view(1,1,-1) #np.reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pytorch, inplace operations have a _ (ex: y.add_(x) modify y)\n",
    "Operation and [] same an numpy\n",
    "\n",
    "globally, if np.function exists, torch.function exists too. Broadcasting is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16 32 48]\n",
      " [64 80 96]]\n",
      "[[ 32  64  96]\n",
      " [128 160 192]]\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = x.numpy()\n",
    "print(z)\n",
    "\n",
    "x.add_(x)\n",
    "print(z) #only on CPU\n",
    "\n",
    "print(x == t.from_numpy(z))\n",
    "print(x==z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't go from torch to numpy with gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(t.version.cuda) # we can easily use gpu device using \"device = 'cuda\"\n",
    "print(t.cuda.is_available())# but we need to have cuda avaible..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.9064, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1732,  0.2491,  0.1589,  0.2192,  0.3208],\n",
       "        [ 0.1641,  0.1684,  0.1251,  0.0942,  0.2367],\n",
       "        [ 0.1601,  0.2553,  0.1504,  0.2413,  0.1020],\n",
       "        [ 0.1246,  0.0893,  0.1092, -0.0183, -0.0067],\n",
       "        [ 0.2253,  0.1367,  0.1222,  0.2316,  0.1583]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(5,5, requires_grad=True) #saying we will need the gradient\n",
    "\n",
    "x = a + 2\n",
    "x = x**2\n",
    "x = x.mean()\n",
    "print(x)\n",
    "x.backward() # can only work with scalar element\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.8662, 14.6762,  8.9028,  2.1253,  0.9520],\n",
      "        [ 3.0395, 14.8620,  2.2105,  2.4313,  5.8787],\n",
      "        [ 1.4909, 10.9141,  1.8609, 12.2093,  7.6037],\n",
      "        [ 8.3279, 10.5153,  0.2546,  7.0688,  3.3500],\n",
      "        [ 2.3875,  9.6000, 10.1063,  1.0823,  1.6036]], grad_fn=<PowBackward0>)\n",
      "tensor([[-0.1327, 14.0285,  5.8705, -1.5808, -1.9988],\n",
      "        [-0.8947, 14.3034, -1.5261, -1.3745,  2.0590],\n",
      "        [-1.9023,  8.6136, -1.7348, 10.4419,  4.1775],\n",
      "        [ 5.1126,  8.0597, -1.5092,  3.5027, -0.6212],\n",
      "        [-1.4056,  6.8064,  7.4965, -1.9967, -1.8581]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(5,5, requires_grad=True) #saying we will need the gradient\n",
    "\n",
    "x = a + 2\n",
    "x = x**2\n",
    "print(x)\n",
    "x.backward(a) #carefull : calling backward successfully will add the attributes in a.grad\n",
    "print(a.grad)\n",
    "\n",
    "a.grad.zero_() # to reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5721, 12.1949, 15.4820,  1.4040,  5.1724],\n",
      "        [ 0.3754, 12.0809,  2.0509,  7.8276,  1.7297],\n",
      "        [ 0.1830,  0.9580,  3.5856,  5.5274,  4.8803],\n",
      "        [ 4.0128,  2.4643,  0.0166,  2.2048,  4.4744],\n",
      "        [ 5.4705,  3.6380,  1.0603,  0.2268,  7.0697]], grad_fn=<PowBackward0>)\n",
      "tensor([[ 3.5721, 14.1949, 17.4820,  3.4040,  7.1724],\n",
      "        [ 2.3754, 14.0809,  4.0509,  9.8276,  3.7297],\n",
      "        [ 2.1830,  2.9580,  5.5856,  7.5274,  6.8803],\n",
      "        [ 6.0128,  4.4643,  2.0166,  4.2048,  6.4744],\n",
      "        [ 7.4705,  5.6380,  3.0603,  2.2268,  9.0697]])\n"
     ]
    }
   ],
   "source": [
    "# get rude of the gradiant constraint\n",
    "\n",
    "x.requires_grad_(False)\n",
    "#or\n",
    "x.detach()\n",
    "#or\n",
    "with t.no_grad():\n",
    "    y = x+ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], dtype=torch.float16, requires_grad=True)\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]], dtype=torch.float16, grad_fn=<ViewBackward0>)\n",
      "tensor([1., 2., 3.], dtype=torch.float16) tensor([1.]) None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.tensor(1, dtype=t.float16)\n",
    "x = t.tensor([1,2,3], dtype=t.float16).view(-1,1)\n",
    "\n",
    "w = t.tensor([1,2,3], dtype=t.float16, requires_grad=True)\n",
    "print(w)\n",
    "print(w.reshape(-1,1)) #reshape is allready a function to proceed !\n",
    "\n",
    "b = t.ones(1, requires_grad=True)\n",
    "\n",
    "z= w.reshape(1,-1) @ x + b\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(w.grad, b.grad, x.grad)\n",
    "\n",
    "w.grad.zero_()\n",
    "b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1-th, loss = 10.20376\n",
      "epoch 11-th, loss = 9.36924\n",
      "epoch 21-th, loss = 8.61502\n",
      "epoch 31-th, loss = 7.93310\n",
      "epoch 41-th, loss = 7.31630\n",
      "epoch 51-th, loss = 6.75817\n",
      "epoch 61-th, loss = 6.25288\n",
      "epoch 71-th, loss = 5.79520\n",
      "epoch 81-th, loss = 5.38042\n",
      "epoch 91-th, loss = 5.00430\n",
      "epoch 101-th, loss = 4.66303\n",
      "epoch 111-th, loss = 4.35317\n",
      "epoch 121-th, loss = 4.07163\n",
      "epoch 131-th, loss = 3.81564\n",
      "epoch 141-th, loss = 3.58268\n",
      "epoch 151-th, loss = 3.37050\n",
      "epoch 161-th, loss = 3.17708\n",
      "epoch 171-th, loss = 3.00059\n",
      "epoch 181-th, loss = 2.83938\n",
      "epoch 191-th, loss = 2.69197\n",
      "epoch 201-th, loss = 2.55704\n",
      "epoch 211-th, loss = 2.43338\n",
      "epoch 221-th, loss = 2.31990\n",
      "epoch 231-th, loss = 2.21564\n",
      "epoch 241-th, loss = 2.11972\n",
      "epoch 251-th, loss = 2.03135\n",
      "epoch 261-th, loss = 1.94982\n",
      "epoch 271-th, loss = 1.87448\n",
      "epoch 281-th, loss = 1.80475\n",
      "epoch 291-th, loss = 1.74012\n",
      "epoch 301-th, loss = 1.68012\n",
      "epoch 311-th, loss = 1.62432\n",
      "epoch 321-th, loss = 1.57234\n",
      "epoch 331-th, loss = 1.52384\n",
      "epoch 341-th, loss = 1.47850\n",
      "epoch 351-th, loss = 1.43604\n",
      "epoch 361-th, loss = 1.39622\n",
      "epoch 371-th, loss = 1.35879\n",
      "epoch 381-th, loss = 1.32357\n",
      "epoch 391-th, loss = 1.29035\n",
      "epoch 401-th, loss = 1.25897\n",
      "epoch 411-th, loss = 1.22928\n",
      "epoch 421-th, loss = 1.20114\n",
      "epoch 431-th, loss = 1.17442\n",
      "epoch 441-th, loss = 1.14902\n",
      "epoch 451-th, loss = 1.12483\n",
      "epoch 461-th, loss = 1.10175\n",
      "epoch 471-th, loss = 1.07971\n",
      "epoch 481-th, loss = 1.05862\n",
      "epoch 491-th, loss = 1.03842\n",
      "epoch 501-th, loss = 1.01904\n",
      "epoch 511-th, loss = 1.00043\n",
      "epoch 521-th, loss = 0.98253\n",
      "epoch 531-th, loss = 0.96531\n",
      "epoch 541-th, loss = 0.94870\n",
      "epoch 551-th, loss = 0.93268\n",
      "epoch 561-th, loss = 0.91721\n",
      "epoch 571-th, loss = 0.90225\n",
      "epoch 581-th, loss = 0.88778\n",
      "epoch 591-th, loss = 0.87377\n",
      "epoch 601-th, loss = 0.86019\n",
      "epoch 611-th, loss = 0.84702\n",
      "epoch 621-th, loss = 0.83424\n",
      "epoch 631-th, loss = 0.82182\n",
      "epoch 641-th, loss = 0.80976\n",
      "epoch 651-th, loss = 0.79802\n",
      "epoch 661-th, loss = 0.78661\n",
      "epoch 671-th, loss = 0.77550\n",
      "epoch 681-th, loss = 0.76467\n",
      "epoch 691-th, loss = 0.75413\n",
      "epoch 701-th, loss = 0.74385\n",
      "epoch 711-th, loss = 0.73383\n",
      "epoch 721-th, loss = 0.72405\n",
      "epoch 731-th, loss = 0.71451\n",
      "epoch 741-th, loss = 0.70519\n",
      "epoch 751-th, loss = 0.69609\n",
      "epoch 761-th, loss = 0.68720\n",
      "epoch 771-th, loss = 0.67852\n",
      "epoch 781-th, loss = 0.67003\n",
      "epoch 791-th, loss = 0.66174\n",
      "epoch 801-th, loss = 0.65363\n",
      "epoch 811-th, loss = 0.64569\n",
      "epoch 821-th, loss = 0.63793\n",
      "epoch 831-th, loss = 0.63033\n",
      "epoch 841-th, loss = 0.62290\n",
      "epoch 851-th, loss = 0.61563\n",
      "epoch 861-th, loss = 0.60850\n",
      "epoch 871-th, loss = 0.60153\n",
      "epoch 881-th, loss = 0.59470\n",
      "epoch 891-th, loss = 0.58801\n",
      "epoch 901-th, loss = 0.58146\n",
      "epoch 911-th, loss = 0.57504\n",
      "epoch 921-th, loss = 0.56875\n",
      "epoch 931-th, loss = 0.56258\n",
      "epoch 941-th, loss = 0.55654\n",
      "epoch 951-th, loss = 0.55062\n",
      "epoch 961-th, loss = 0.54481\n",
      "epoch 971-th, loss = 0.53912\n",
      "epoch 981-th, loss = 0.53354\n",
      "epoch 991-th, loss = 0.52806\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "X = t.eye(3, dtype=t.float32)\n",
    "X[0,1] = 2.0\n",
    "w = t.zeros(4,3, requires_grad=True)\n",
    "b= t.zeros(4,1, requires_grad=True)\n",
    "f = lambda x : 4 * x[0] - 2 * x[1] + 3 # f= 4x-2y (+0z) +3\n",
    "target = t.tensor([[f(inst)] for inst in X], dtype=t.float32)\n",
    "\n",
    "# def forward(x):\n",
    "#     return t.sigmoid(w @ x.T + b).mean(dim=0)\n",
    "\n",
    "model = nn.Linear(3,1)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = t.optim.SGD(model.parameters(), lr= 0.01)\n",
    "\n",
    "n_iter = 1000\n",
    "\n",
    "for epoch in range(n_iter):\n",
    "    y_pred = model(X)\n",
    "\n",
    "    l = loss(y_pred, target)\n",
    "\n",
    "    l.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # with t.no_grad():\n",
    "    #     w -= lr * w.grad\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        print(f\"epoch {epoch+1}-th, loss = {l:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE DATALOADER\n",
    "\n",
    "```python\n",
    "from torch.utils.data import Dataset, Dataloader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init_(self):\n",
    "        # data loading\n",
    "        # no super().__init__()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "\n",
    "dataset = MyDataset()\n",
    "dataloader = DataLoader(dataset = dataset, batch_size=4, shuffle=True, num_workers = to have multiply processing )\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
